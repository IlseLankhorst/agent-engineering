{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first take-home project will be a chance to get some hands-on practice with the concepts of Week 1 for an agent task of your choice.\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Choose a task involving multi-turn interaction/tool use\n",
    "- Implement an agent scaffold either using an API directly, or one of the frameworks we covered\n",
    "- Create a small set of \"test prompts\"\n",
    "- Create a \"reward function\" to evaluate your agent\n",
    "- Test your agent setup with multiple models/prompts\n",
    "- Examine multiple agent outputs, identify a consistent \"problem\", adjust the setup (prompts/tools) OR adjust your evals to measure/address the problem\n",
    "\n",
    "Ideas for agent tasks:\n",
    "- Search agent for your favorite blog/website\n",
    "- Agent which\n",
    "- Agent for playing a simple board/card game\n",
    "- Code agent specialized to only use a specific library, e.g. iterating on a matplotlib plot until it \"looks right\"\n",
    "- Terminal-based chat agent with user handoff/confirmation\n",
    "\n",
    "Ideas for reward functions:\n",
    "- Format checks using regex\n",
    "- Deterministic checks (parsing math answers, running code with test cases, solving a puzzle/game)\n",
    "- Embedding or text overlap similarity to a \"ground truth\"\n",
    "- LLM judges which can see the \"ground truth\"\n",
    "- LLM judges which evaluate a set of fuzzy criteria + give scores for each\n",
    "\n",
    "Tips:\n",
    "- Start simple, get a basic version working, then ramp up complexity\n",
    "- If your agent \"just works\" with a fairly powerful model, try it with a weaker model and see what breaks\n",
    "\n",
    "\n",
    "Bonus goals:\n",
    "- Try making a \"parallel-friendly\" version using asyncio + error handling\n",
    "- Try implementing Best-of-N selection -- can your eval function match your judgment for which outputs are \"best\"?\n",
    "- Try testing either a \"multi-agent\" (parallelized) version of your agent, OR a Client/Server version (e.g. MCP, A2A)\n",
    "\n",
    "\n",
    "Deliverable:\n",
    "- A repo, notebook, *or* short writeup detailing your setup + experimentation\n",
    "- What approaches did you try?\n",
    "- What roadblocks did you run into?\n",
    "- Which evaluation methods worked best for your task?\n",
    "- What's the smallest model that worked decently well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# simplest possible LLM call\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "oai = OpenAI()\n",
    "\n",
    "response = oai.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "class SimpleLineageAgent:\n",
    "    \"\"\"Basic lineage agent using only LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"gpt-4.1-mini\"):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "    \n",
    "    def analyze_lineage(self, code: str, target_variable: str) -> str:\n",
    "        \"\"\"Analyze lineage using only LLM\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"You are a data engineer which needs to analyze the lineage of a target variable to refactor the code. \n",
    "        \n",
    "        Analyze the code and explain how the target variable was created:\n",
    "        1. What source tables/files were used\n",
    "        2. What operations were performed \n",
    "        3. The step-by-step data flow\n",
    "        \n",
    "        Be clear and concise.\"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "        Variable to analyze: {target_variable}\n",
    "        \n",
    "        Code:\n",
    "        ```\n",
    "        {code}\n",
    "        ```\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Simple Lineage Analysis ===\n",
      "Code:\n",
      "\n",
      "# Load data\n",
      "transactions = spark.table(\"raw.transactions\")\n",
      "customers = spark.table(\"raw.customers\")\n",
      "\n",
      "# Filter recent transactions\n",
      "recent_tx = transactions.filter(col(\"date\") >= \"2023-01-01\")\n",
      "\n",
      "# Group by customer\n",
      "customer_totals = recent_tx.groupBy(\"customer_id\").sum(\"amount\")\n",
      "\n",
      "# Join with customer names\n",
      "final_result = customer_totals.join(customers, \"customer_id\")\n",
      "\n",
      "\n",
      "==================================================\n",
      "Analyzing 'final_result':\n",
      "1. Source tables/files used:\n",
      "   - \"raw.transactions\" table\n",
      "   - \"raw.customers\" table\n",
      "\n",
      "2. Operations performed:\n",
      "   - Load the two tables into Spark DataFrames.\n",
      "   - Filter the transactions to keep only those from January 1, 2023, onwards.\n",
      "   - Group the filtered transactions by customer_id and sum the amount for each customer.\n",
      "   - Join the aggregated transaction totals with the customers table on customer_id.\n",
      "\n",
      "3. Step-by-step data flow:\n",
      "   - Load \"raw.transactions\" into `transactions`.\n",
      "   - Load \"raw.customers\" into `customers`.\n",
      "   - Filter `transactions` to create `recent_tx` with transactions dated >= \"2023-01-01\".\n",
      "   - Aggregate `recent_tx` by customer_id, summing the amount column, resulting in `customer_totals`.\n",
      "   - Join `customer_totals` with `customers` on customer_id to produce `final_result`, which contains customer details along with their total transaction amounts since 2023-01-01.\n",
      "\n",
      "------------------------------\n",
      "Analyzing 'customer_totals':\n",
      "1. Source tables/files used:\n",
      "   - `raw.transactions` table\n",
      "   - `raw.customers` table\n",
      "\n",
      "2. Operations performed:\n",
      "   - Load the `transactions` and `customers` tables into Spark DataFrames.\n",
      "   - Filter the `transactions` DataFrame to keep only records with a `date` on or after \"2023-01-01\".\n",
      "   - Group the filtered transactions by `customer_id` and calculate the sum of the `amount` for each customer.\n",
      "   - Join the aggregated totals with the `customers` DataFrame on `customer_id`.\n",
      "\n",
      "3. Step-by-step data flow:\n",
      "   - Read `raw.transactions` into `transactions`.\n",
      "   - Read `raw.customers` into `customers`.\n",
      "   - Filter `transactions` to create `recent_tx` containing only transactions from 2023 onwards.\n",
      "   - Aggregate `recent_tx` by `customer_id` to compute total transaction amounts, resulting in `customer_totals`.\n",
      "   - Join `customer_totals` with `customers` on `customer_id` to enrich totals with customer details, producing `final_result`.\n",
      "\n",
      "The target variable `customer_totals` specifically represents the sum of transaction amounts per customer for transactions dated from 2023-01-01 onwards.\n"
     ]
    }
   ],
   "source": [
    "# Simple test function\n",
    "\n",
    "sample_code = '''\n",
    "# Load data\n",
    "transactions = spark.table(\"raw.transactions\")\n",
    "customers = spark.table(\"raw.customers\")\n",
    "\n",
    "# Filter recent transactions\n",
    "recent_tx = transactions.filter(col(\"date\") >= \"2023-01-01\")\n",
    "\n",
    "# Group by customer\n",
    "customer_totals = recent_tx.groupBy(\"customer_id\").sum(\"amount\")\n",
    "\n",
    "# Join with customer names\n",
    "final_result = customer_totals.join(customers, \"customer_id\")\n",
    "'''\n",
    "    \n",
    "agent = SimpleLineageAgent()\n",
    "\n",
    "print(\"=== Simple Lineage Analysis ===\")\n",
    "print(f\"Code:\\n{sample_code}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test analyzing final_result\n",
    "print(\"Analyzing 'final_result':\")\n",
    "result = agent.analyze_lineage(sample_code, \"final_result\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "\n",
    "# Test analyzing intermediate variable\n",
    "print(\"Analyzing 'customer_totals':\")\n",
    "result2 = agent.analyze_lineage(sample_code, \"customer_totals\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Simple Lineage Analysis ===\n",
      "Code:\n",
      "\n",
      "# Load data\n",
      "transactions = spark.table(\"raw.transactions\")\n",
      "customers = spark.table(\"raw.customers\")\n",
      "\n",
      "# Filter recent transactions\n",
      "recent_tx = transactions.filter(col(\"date\") >= \"2023-01-01\")\n",
      "\n",
      "# Group by customer\n",
      "customer_totals = recent_tx.groupBy(\"customer_id\").sum(\"amount\")\n",
      "\n",
      "# Join with customer names\n",
      "final_result = customer_totals.join(customers, \"customer_id\")\n",
      "\n",
      "\n",
      "==================================================\n",
      "Analyzing 'final_result':\n",
      "The target variable `final_result` was created through the following data flow:\n",
      "\n",
      "1. **Source Tables:**\n",
      "   - `raw.transactions`: Contains transaction data, including `customer_id`, `amount`, and `date`.\n",
      "   - `raw.customers`: Contains customer details, including `customer_id` and customer name information.\n",
      "\n",
      "2. **Operations Performed:**\n",
      "   - Filtered `transactions` to include only recent transactions from January 1, 2023, onwards (`recent_tx`).\n",
      "   - Aggregated `recent_tx` by `customer_id` to compute the total transaction amount per customer (`customer_totals`).\n",
      "   - Joined `customer_totals` with `customers` on `customer_id` to associate customer details with their total transaction amounts.\n",
      "\n",
      "3. **Data Flow Step-by-Step:**\n",
      "   - Load `transactions` and `customers` tables.\n",
      "   - Filter `transactions` for dates >= 2023-01-01.\n",
      "   - Group the filtered transactions by `customer_id` and sum the `amount` to get total amounts per customer.\n",
      "   - Join the aggregated totals with the `customers` table to include customer information.\n",
      "   - The resulting DataFrame, `final_result`, contains customer details along with their total transaction amount since the specified date.\n",
      "\n",
      "------------------------------\n",
      "Analyzing 'customer_totals':\n",
      "The target variable `customer_totals` was created through the following data flow:\n",
      "\n",
      "1. **Source Tables Used:**\n",
      "   - `raw.transactions`: Contains transaction records, including `customer_id`, `amount`, and `date`.\n",
      "   - `raw.customers`: Contains customer details, including `customer_id` and customer name.\n",
      "\n",
      "2. **Operations Performed:**\n",
      "   - Filtering: The `transactions` table is filtered to include only recent transactions with `date` on or after \"2023-01-01\".\n",
      "   - Aggregation: The filtered transactions (`recent_tx`) are grouped by `customer_id`, and the sum of `amount` is calculated for each customer, resulting in `customer_totals`.\n",
      "\n",
      "3. **Data Flow Step-by-Step:**\n",
      "   - Load `transactions` and `customers` tables.\n",
      "   - Filter `transactions` to keep only recent transactions (`recent_tx`).\n",
      "   - Group `recent_tx` by `customer_id` and compute the total `amount` per customer, creating `customer_totals`.\n",
      "   - The `customer_totals` table is then joined with `customers` to associate customer names, resulting in `final_result`.\n",
      "\n",
      "**Note:** The creation of `customer_totals` involves filtering and aggregation of transaction data, but it does not include the join with customer details.\n"
     ]
    }
   ],
   "source": [
    "# Simple test function\n",
    "\n",
    "sample_code = '''\n",
    "# Load data\n",
    "transactions = spark.table(\"raw.transactions\")\n",
    "customers = spark.table(\"raw.customers\")\n",
    "\n",
    "# Filter recent transactions\n",
    "recent_tx = transactions.filter(col(\"date\") >= \"2023-01-01\")\n",
    "\n",
    "# Group by customer\n",
    "customer_totals = recent_tx.groupBy(\"customer_id\").sum(\"amount\")\n",
    "\n",
    "# Join with customer names\n",
    "final_result = customer_totals.join(customers, \"customer_id\")\n",
    "'''\n",
    "    \n",
    "agent = SimpleLineageAgent(model=\"gpt-4.1-nano\")\n",
    "\n",
    "print(\"=== Simple Lineage Analysis ===\")\n",
    "print(f\"Code:\\n{sample_code}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test analyzing final_result\n",
    "print(\"Analyzing 'final_result':\")\n",
    "result = agent.analyze_lineage(sample_code, \"final_result\")\n",
    "print(result)\n",
    "\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "\n",
    "# Test analyzing intermediate variable\n",
    "print(\"Analyzing 'customer_totals':\")\n",
    "result2 = agent.analyze_lineage(sample_code, \"customer_totals\")\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AGENT WITH SYSTEM PROMPT + TOOLS ===\n",
      "**Sources:**\n",
      "- The data for `sales` comes from the CSV file \"sales_data.csv.\"\n",
      "- The data for `customers` comes from the CSV file \"customers.csv.\"\n",
      "\n",
      "**Operations:**\n",
      "- The `sales` data is filtered to include only recent sales from 2023-01-01 onwards.\n",
      "- The filtered sales data (`recent_sales`) is grouped by `customer_id`, summing the `amount` for each customer, producing `customer_totals`.\n",
      "- The `customer_totals` data is then merged with the `customers` data on `customer_id` to produce the final result.\n",
      "\n",
      "**Flow:**\n",
      "1. Data is loaded from two CSV files: sales and customers.\n",
      "2. From the sales data, only recent sales (from 2023-01-01) are selected.\n",
      "3. The recent sales are grouped by customer, summing the amounts to get total sales per customer.\n",
      "4. These totals are merged with customer details, combining sales summaries with customer information.\n",
      "5. The result, `final_result`, contains each customer’s details along with their total sales for 2023 onwards.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner, function_tool\n",
    "from openai import OpenAI\n",
    "\n",
    "@function_tool\n",
    "async def extract_sources_tool(code: str) -> list[str]:\n",
    "    \"\"\"Extract data sources from code\n",
    "    \n",
    "    Args:\n",
    "        code (str): The code to analyze\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: List of data sources found (files, tables, etc.)\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        r'pd\\.read_csv\\s*\\(\\s*[\"\\']([^\"\\']+)[\"\\']',\n",
    "        r'spark\\.table\\s*\\(\\s*[\"\\']([^\"\\']+)[\"\\']',\n",
    "        r'pd\\.read_parquet\\s*\\(\\s*[\"\\']([^\"\\']+)[\"\\']'\n",
    "    ]\n",
    "    \n",
    "    sources = []\n",
    "    for pattern in patterns:\n",
    "        matches = re.findall(pattern, code)\n",
    "        sources.extend(matches)\n",
    "    \n",
    "    return list(set(sources))\n",
    "\n",
    "@function_tool\n",
    "async def extract_operations_tool(code: str) -> list[str]:\n",
    "    \"\"\"Extract operations from code\n",
    "    \n",
    "    Args:\n",
    "        code (str): The code to analyze\n",
    "    \n",
    "    Returns:\n",
    "        list[str]: List of operations found (filter, groupby, join, etc.)\n",
    "    \"\"\"\n",
    "    operations = []\n",
    "    \n",
    "    if '.filter(' in code or '.query(' in code:\n",
    "        operations.append('filter')\n",
    "    if '.groupby(' in code or '.groupBy(' in code:\n",
    "        operations.append('groupby') \n",
    "    if '.join(' in code or '.merge(' in code:\n",
    "        operations.append('join')\n",
    "    if '.sum(' in code:\n",
    "        operations.append('sum')\n",
    "    if '.mean(' in code:\n",
    "        operations.append('mean')\n",
    "        \n",
    "    return operations\n",
    "\n",
    "lineage_agent = Agent(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    name=\"advanced_lineage_agent\",\n",
    "    \n",
    "    # SYSTEM PROMPT: Instructions for HOW to behave\n",
    "    instructions=\"\"\"You are an expert data lineage analyst. Your job is to help users understand how their data flows through code.\n",
    "\n",
    "    **Your approach:**\n",
    "    1. Always use your tools to extract technical details first\n",
    "    2. Think step-by-step about the data flow\n",
    "    3. Explain things clearly for both technical and non-technical users\n",
    "    4. Be thorough but concise\n",
    "    \n",
    "    **When analyzing code:**\n",
    "    - First, use extract_sources_tool to find all data sources\n",
    "    - Then, use extract_operations_tool to find all operations\n",
    "    - Finally, synthesize this information into a clear explanation\n",
    "    \n",
    "    **Always think step-by-step before calling a tool.**\n",
    "    \n",
    "    Format your final answer with:\n",
    "    - Sources: What data comes from\n",
    "    - Operations: What happens to the data  \n",
    "    - Flow: Step-by-step explanation\"\"\",\n",
    "    \n",
    "    # TOOLS: What the agent CAN DO\n",
    "    tools=[extract_sources_tool, extract_operations_tool],\n",
    ")\n",
    "\n",
    "sample_code = '''\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "sales = pd.read_csv(\"sales_data.csv\")\n",
    "customers = pd.read_csv(\"customers.csv\")\n",
    "\n",
    "# Process data\n",
    "recent_sales = sales.query(\"date >= '2023-01-01'\")\n",
    "customer_totals = recent_sales.groupby('customer_id')['amount'].sum()\n",
    "final_result = customer_totals.merge(customers, on='customer_id')\n",
    "'''\n",
    "    \n",
    "prompt = f\"\"\"\n",
    "Analyze the lineage of the variable 'final_result' in this code:\n",
    "\n",
    "{sample_code}\n",
    "\n",
    "I need to understand where the data comes from and how it's transformed.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== AGENT WITH SYSTEM PROMPT + TOOLS ===\")\n",
    "result = await Runner.run(lineage_agent, prompt)\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: basic_pyspark_filter\n",
      "  Reward Score: 1.00\n",
      "  Expected: sources=['users'], ops=['filter']\n",
      "  Agent response (first 200 chars): Let's analyze the data lineage step-by-step:\n",
      "\n",
      "- **Sources**: The data for `filtered_df` ultimately originates from the \"users\" table, which is read into the variable `df`.\n",
      "\n",
      "- **Operations**:\n",
      "  - First...\n",
      "------------------------------------------------------------\n",
      "Test 2: basic_pandas_filter\n",
      "  Reward Score: 1.00\n",
      "  Expected: sources=['sales.csv'], ops=['filter']\n",
      "  Agent response (first 200 chars): **Sources:**\n",
      "- The data source for `high_sales` is the CSV file named \"sales.csv\".\n",
      "\n",
      "**Operations:**\n",
      "- The variable `high_sales` is created by filtering the DataFrame `df`.\n",
      "- The filtering operation se...\n",
      "------------------------------------------------------------\n",
      "Test 3: pandas_groupby_merge\n",
      "  Reward Score: 0.57\n",
      "  Expected: sources=['orders.csv', 'customers.csv', 'products.csv'], ops=['read_csv', 'dropna', 'groupby', 'agg_sum', 'agg_count', 'reset_index', 'merge']\n",
      "  Agent response (first 200 chars): **Sources:**\n",
      "- The data originates from three CSV files: `orders.csv`, `customers.csv`, and `products.csv`.\n",
      "\n",
      "**Operations:**\n",
      "- The data from `orders.csv` is cleaned by removing rows with missing `cust...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from test_cases import TEST_CASES\n",
    "\n",
    "def simple_reward(agent_response: str, expected_sources: list, expected_operations: list) -> float:\n",
    "    \"\"\"Simple reward function that scores agent response\"\"\"\n",
    "    response_lower = agent_response.lower()\n",
    "    \n",
    "    # Count how many expected sources are mentioned\n",
    "    sources_found = sum(1 for source in expected_sources \n",
    "                       if source.lower() in response_lower)\n",
    "    source_score = sources_found / len(expected_sources) if expected_sources else 1.0\n",
    "    \n",
    "    # Count how many expected operations are mentioned  \n",
    "    ops_found = sum(1 for op in expected_operations \n",
    "                   if op.replace('_', ' ').lower() in response_lower)\n",
    "    op_score = ops_found / len(expected_operations) if expected_operations else 1.0\n",
    "    \n",
    "    return (source_score + op_score) / 2\n",
    "\n",
    "for i, test_case in enumerate(TEST_CASES[:3]):  # First 3 cases\n",
    "    print(f\"Test {i+1}: {test_case['name']}\")\n",
    "    \n",
    "    # Get agent response\n",
    "    prompt = f\"Analyze the lineage of variable '{test_case['target']}' in this code:\\n\\n{test_case['code']}\"\n",
    "    result = await Runner.run(lineage_agent, prompt)    \n",
    "\n",
    "    # Calculate reward\n",
    "    reward = simple_reward(result.final_output, \n",
    "                          test_case['expected_sources'], \n",
    "                          test_case['expected_operations'])\n",
    "    \n",
    "    print(f\"  Reward Score: {reward:.2f}\")\n",
    "    print(f\"  Expected: sources={test_case['expected_sources']}, ops={test_case['expected_operations']}\")\n",
    "    print(f\"  Agent response (first 200 chars): {result.final_output[:200]}...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge Agent Test 1: basic_pyspark_filter\n",
      "  Judge Agent Score: 1.00\n",
      "  Lineage Agent said: **Sources:**  \n",
      "- The data for `filtered_df` originates from the data source `users`, which appears to be a table in a Spark environment.\n",
      "\n",
      "**Operations:**  \n",
      "- The main operation is a filter operation applied to the `df` DataFrame, selecting only the records where the `age` column value is greater than 18.\n",
      "\n",
      "**Flow:**  \n",
      "1. The data is loaded from the `users` table into the DataFrame `df`.  \n",
      "2. The DataFrame `df` undergoes a filtering operation, resulting in `filtered_df`.  \n",
      "3. `filtered_df` contains only the records from `users` where the `age` exceeds 18.  \n",
      "\n",
      "This process narrows down the original `users` dataset to include only adult users.\n",
      "--------------------------------------------------\n",
      "Judge Agent Test 2: basic_pandas_filter\n",
      "  Judge Agent Score: 0.80\n",
      "  Lineage Agent said: **Sources:**  \n",
      "The variable `high_sales` is derived from the data source `sales.csv`, which is a CSV file containing sales data.\n",
      "\n",
      "**Operations:**  \n",
      "1. The raw data is loaded from `sales.csv` into a DataFrame named `df`.  \n",
      "2. The variable `high_sales` is then created by filtering the `df` DataFrame to include only rows where the `amount` column is greater than 1000.\n",
      "\n",
      "**Flow:**  \n",
      "- Data flows into the process from the file `sales.csv`.  \n",
      "- The CSV file's data is loaded into a pandas DataFrame (`df`).  \n",
      "- The `high_sales` variable is assigned a filtered subset of `df`, specifically where sales amounts exceed 1000.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: LLM Judge Agent with Same Tools\n",
    "judge_agent = Agent(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    name=\"lineage_judge_agent\",\n",
    "    \n",
    "    instructions=\"\"\"You are an expert judge for data lineage analysis. Your job is to evaluate how well another agent analyzed code lineage.\n",
    "\n",
    "    **Your approach:**\n",
    "    1. First, use your tools to find the ground truth (actual sources and operations in the code)\n",
    "    2. Compare the agent's response against both the ground truth AND the expected results\n",
    "    3. Give a fair, objective score from 0-10\n",
    "    \n",
    "    **When judging:**\n",
    "    - Use extract_sources_tool to find all actual data sources\n",
    "    - Use extract_operations_tool to find all actual operations  \n",
    "    - Compare what the agent found vs. what actually exists\n",
    "    - Consider accuracy, completeness, and clarity\n",
    "    \n",
    "    **Always use your tools first, then provide your judgment.**\n",
    "    \n",
    "    Format your response as:\n",
    "    SCORE: X/10\n",
    "    EXPLANATION: explanation of the score. What finds the judge agent which was not in the expected results\"\"\",\n",
    "    \n",
    "    tools=[extract_sources_tool, extract_operations_tool],\n",
    ")\n",
    "\n",
    "async def agent_judge(code: str, target: str, agent_response: str, expected_sources: list, expected_operations: list) -> float:\n",
    "    \"\"\"Use judge agent to evaluate the lineage agent's response\"\"\"\n",
    "    \n",
    "    judge_prompt = f\"\"\"Please evaluate this lineage analysis:\n",
    "\n",
    "TARGET VARIABLE: {target}\n",
    "\n",
    "CODE TO ANALYZE:\n",
    "{code}\n",
    "\n",
    "EXPECTED RESULTS:\n",
    "- Sources: {expected_sources}\n",
    "- Operations: {expected_operations}\n",
    "\n",
    "AGENT'S RESPONSE:\n",
    "{agent_response}\n",
    "\n",
    "Please use your tools to find the ground truth, then score the agent's response from 0-10.\"\"\"\n",
    "\n",
    "    # Run the judge agent\n",
    "    judge_result = await Runner.run(judge_agent, judge_prompt)\n",
    "    \n",
    "    # Extract score from judge response\n",
    "    try:\n",
    "        response_text = judge_result.final_output.upper()\n",
    "        if \"SCORE:\" in response_text:\n",
    "            score_line = [line for line in response_text.split('\\n') if 'SCORE:' in line][0]\n",
    "            score = float(score_line.split('SCORE:')[1].split('/')[0].strip())\n",
    "            return score / 10.0  # Convert to 0-1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return 0.5  # Default if parsing fails\n",
    "\n",
    "# Test judge agent on lineage agent responses\n",
    "for i, test_case in enumerate(TEST_CASES[:2]):  # First 2 cases only (API cost)\n",
    "    print(f\"Judge Agent Test {i+1}: {test_case['name']}\")\n",
    "    \n",
    "    # Get lineage agent response\n",
    "    prompt = f\"Analyze the lineage of variable '{test_case['target']}' in this code:\\n\\n{test_case['code']}\"\n",
    "    result = await Runner.run(lineage_agent, prompt)\n",
    "    agent_output = result.final_output\n",
    "    \n",
    "    # Judge the response using judge agent\n",
    "    score = await agent_judge(test_case['code'], test_case['target'], agent_output, \n",
    "                             test_case['expected_sources'], test_case['expected_operations'])\n",
    "    \n",
    "    print(f\"  Judge Agent Score: {score:.2f}\")\n",
    "    print(f\"  Lineage Agent said: {agent_output}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 TESTING CODE ANALYSIS TOOLS\n",
      "==================================================\n",
      "📁 Sources found:\n",
      "['sales_data.csv', 'customers.csv']\n",
      "\n",
      "📝 Variable assignments:\n",
      "  sales: lines [5]\n",
      "  customers: lines [6]\n",
      "  recent_sales: lines [9]\n",
      "  customer_totals: lines [10]\n",
      "  final_result: lines [11]\n",
      "\n",
      "⚙️ Operations found:\n",
      "  Line 9: filter - recent_sales = sales.query(\"date >= '2023-01-01'\")\n",
      "  Line 10: groupby - customer_totals = recent_sales.groupby('customer_id')['amount'].sum().reset_index()\n",
      "  Line 10: reset_index - customer_totals = recent_sales.groupby('customer_id')['amount'].sum().reset_index()\n",
      "  Line 10: agg_sum - customer_totals = recent_sales.groupby('customer_id')['amount'].sum().reset_index()\n",
      "  Line 10: groupBy - customer_totals = recent_sales.groupby('customer_id')['amount'].sum().reset_index()\n",
      "  Line 10: sql_SUM - customer_totals = recent_sales.groupby('customer_id')['amount'].sum().reset_index()\n",
      "  Line 11: merge - final_result = customer_totals.merge(customers, on='customer_id')\n",
      "\n",
      "🎯 Trace 'final_result':\n",
      "  Sources: ['sales_data.csv', 'customers.csv']\n",
      "  Operations: ['filter', 'merge', 'groupby', 'agg_sum', 'reset_index', 'groupBy', 'sql_SUM']\n",
      "  Assignment lines: [11]\n"
     ]
    }
   ],
   "source": [
    "from tools import CodeAnalysisTools\n",
    "\n",
    "sample_code = \"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "sales = pd.read_csv(\"sales_data.csv\")\n",
    "customers = pd.read_csv(\"customers.csv\")\n",
    "\n",
    "# Process data\n",
    "recent_sales = sales.query(\"date >= '2023-01-01'\")\n",
    "customer_totals = recent_sales.groupby('customer_id')['amount'].sum().reset_index()\n",
    "final_result = customer_totals.merge(customers, on='customer_id')\n",
    "\"\"\"\n",
    "\n",
    "tools = CodeAnalysisTools()\n",
    "\n",
    "print(\"🔧 TESTING CODE ANALYSIS TOOLS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"📁 Sources found:\")\n",
    "sources = tools.extract_table_sources(sample_code)\n",
    "print(sources)\n",
    "\n",
    "print(\"\\n📝 Variable assignments:\")\n",
    "assignments = tools.extract_variable_assignments(sample_code)\n",
    "for var, lines in assignments.items():\n",
    "    print(f\"  {var}: lines {lines}\")\n",
    "\n",
    "print(\"\\n⚙️ Operations found:\")\n",
    "operations = tools.extract_operations(sample_code)\n",
    "for op in operations:\n",
    "    print(f\"  Line {op['line']}: {op['operation']} - {op['code']}\")\n",
    "\n",
    "print(\"\\n🎯 Trace 'final_result':\")\n",
    "trace = tools.trace_variable_dependencies(sample_code, \"final_result\")\n",
    "print(f\"  Sources: {trace['sources']}\")\n",
    "print(f\"  Operations: {trace['operations']}\")\n",
    "print(f\"  Assignment lines: {trace['assignment_lines']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def compare_tool_vs_llm(self, code: str, target_variable: str) -> dict[str, any]:\n",
    "    #     \"\"\"Compare tool analysis vs LLM analysis\"\"\"\n",
    "\n",
    "    #     # Tool analysis\n",
    "    #     tool_result = self.trace_variable_dependencies(code, target_variable)\n",
    "\n",
    "    #     # Pure LLM analysis (without tools)\n",
    "    #     system_prompt = \"\"\"You are a code lineage analysis expert.\n",
    "\n",
    "    #     Analyze the code to trace how the target variable was created:\n",
    "    #     1. What source tables/files were used\n",
    "    #     2. What operations were performed\n",
    "    #     3. The step-by-step flow\"\"\"\n",
    "\n",
    "    #     user_prompt = f\"Target variable: {target_variable}\\n\\nCode:\\n```\\n{code}\\n```\"\n",
    "\n",
    "    #     llm_response = self.client.chat.completions.create(\n",
    "    #         model=self.model,\n",
    "    #         messages=[\n",
    "    #             {\"role\": \"system\", \"content\": system_prompt},\n",
    "    #             {\"role\": \"user\", \"content\": user_prompt},\n",
    "    #         ],\n",
    "    #         temperature=0,\n",
    "    #     )\n",
    "\n",
    "    #     # Enhanced analysis (tools + LLM)\n",
    "    #     enhanced_response = self.analyze_lineage_with_tools(code, target_variable)\n",
    "\n",
    "    #     return {\n",
    "    #         \"tool_analysis\": tool_result,\n",
    "    #         \"llm_only\": llm_response.choices[0].message.content,\n",
    "    #         \"enhanced_analysis\": enhanced_response,\n",
    "    #     }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
